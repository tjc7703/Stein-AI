"""
ğŸ’¾ ë¬´í•œ í™•ì¥ ë©”ëª¨ë¦¬ ì—”ì§„
ëª¨ë“  ê²½í—˜, í•™ìŠµ, í˜ì‹ ì„ ì˜êµ¬ ì €ì¥í•˜ëŠ” ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ

Steinë‹˜ê³¼ì˜ ëª¨ë“  ìˆœê°„ì„ ê¸°ì–µí•˜ê³  ì§€ì†ì ìœ¼ë¡œ ì§„í™”í•˜ëŠ” AIì˜ ë‡Œ
"""

import asyncio
import json
import time
import sqlite3
import hashlib
import pickle
import gzip
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np
from enum import Enum
import threading
from concurrent.futures import ThreadPoolExecutor
import uuid

class MemoryType(Enum):
    """ë©”ëª¨ë¦¬ ìœ í˜•"""
    CONVERSATION = "conversation"
    LEARNING = "learning"
    CODE = "code"
    INNOVATION = "innovation"
    COLLABORATION = "collaboration"
    EMOTION = "emotion"
    INSIGHT = "insight"
    PATTERN = "pattern"

class MemoryImportance(Enum):
    """ë©”ëª¨ë¦¬ ì¤‘ìš”ë„"""
    CRITICAL = 5  # ì ˆëŒ€ ì‚­ì œ ë¶ˆê°€
    HIGH = 4      # ë§¤ìš° ì¤‘ìš”
    MEDIUM = 3    # ë³´í†µ
    LOW = 2       # ë‚®ìŒ
    MINIMAL = 1   # ìµœì†Œ

@dataclass
class MemoryRecord:
    """ë©”ëª¨ë¦¬ ë ˆì½”ë“œ"""
    id: str
    timestamp: str
    memory_type: MemoryType
    importance: MemoryImportance
    content: Dict[str, Any]
    embeddings: Optional[List[float]]
    tags: List[str]
    connections: List[str]  # ì—°ê´€ëœ ë‹¤ë¥¸ ë©”ëª¨ë¦¬ IDë“¤
    access_count: int
    last_accessed: str
    retention_score: float

class InfiniteMemoryEngine:
    """
    ğŸ’¾ ë¬´í•œ í™•ì¥ ë©”ëª¨ë¦¬ ì—”ì§„
    - ëª¨ë“  ê²½í—˜ê³¼ í•™ìŠµì„ ì˜êµ¬ ì €ì¥
    - ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ êµ¬ì¡°í™” ë° ì—°ê´€ì„± ê´€ë¦¬
    - íš¨ìœ¨ì ì¸ ê²€ìƒ‰ ë° íšŒìƒ ì‹œìŠ¤í…œ
    - ìë™ ì¤‘ìš”ë„ í‰ê°€ ë° ë³´ì¡´ ì •ì±…
    """
    
    def __init__(self):
        self.data_path = Path("data/infinite_memory")
        self.data_path.mkdir(parents=True, exist_ok=True)
        
        # SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.db_path = self.data_path / "memory_core.db"
        self.init_database()
        
        # ë©”ëª¨ë¦¬ ìºì‹œ (ë¹ ë¥¸ ì ‘ê·¼ìš©)
        self.memory_cache: Dict[str, MemoryRecord] = {}
        self.cache_size_limit = 1000
        
        # ì—°ê´€ì„± ê·¸ë˜í”„
        self.association_graph: Dict[str, Dict[str, float]] = {}
        
        # ê²€ìƒ‰ ì¸ë±ìŠ¤
        self.search_index: Dict[str, List[str]] = {}
        
        # ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.memory_manager_active = True
        self.compression_threshold = 10000  # ë©”ëª¨ë¦¬ ì••ì¶• ì„ê³„ì 
        
        # ë°±ê·¸ë¼ìš´ë“œ ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ë ˆë“œ
        self.manager_thread = threading.Thread(target=self._memory_management_loop, daemon=True)
        self.manager_thread.start()
        
        # ê¸°ì¡´ ë©”ëª¨ë¦¬ ë¡œë“œ
        self._load_existing_memories()
        
        print("ğŸ’¾ ë¬´í•œ í™•ì¥ ë©”ëª¨ë¦¬ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ğŸ“Š í˜„ì¬ ì €ì¥ëœ ë©”ëª¨ë¦¬: {len(self.memory_cache)}ê°œ")
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ë©”ëª¨ë¦¬ í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS memories (
                    id TEXT PRIMARY KEY,
                    timestamp TEXT,
                    memory_type TEXT,
                    importance INTEGER,
                    content_json TEXT,
                    embeddings_blob BLOB,
                    tags_json TEXT,
                    connections_json TEXT,
                    access_count INTEGER DEFAULT 0,
                    last_accessed TEXT,
                    retention_score REAL DEFAULT 1.0
                )
            """)
            
            # ì—°ê´€ì„± í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS associations (
                    memory_id_1 TEXT,
                    memory_id_2 TEXT,
                    strength REAL,
                    created_at TEXT,
                    PRIMARY KEY (memory_id_1, memory_id_2)
                )
            """)
            
            # ê²€ìƒ‰ ì¸ë±ìŠ¤ í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS search_index (
                    keyword TEXT,
                    memory_id TEXT,
                    relevance REAL,
                    PRIMARY KEY (keyword, memory_id)
                )
            """)
            
            conn.commit()
    
    def store_memory(self, 
                    content: Dict[str, Any],
                    memory_type: MemoryType = MemoryType.CONVERSATION,
                    importance: MemoryImportance = MemoryImportance.MEDIUM,
                    tags: List[str] = None) -> str:
        """
        ğŸ’¾ ë©”ëª¨ë¦¬ ì €ì¥
        """
        if tags is None:
            tags = []
        
        # ë©”ëª¨ë¦¬ ID ìƒì„±
        memory_id = str(uuid.uuid4())
        
        # ì„ë² ë”© ìƒì„± (ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜)
        content_str = json.dumps(content, ensure_ascii=False)
        embeddings = self._generate_embeddings(content_str)
        
        # ë©”ëª¨ë¦¬ ë ˆì½”ë“œ ìƒì„±
        memory = MemoryRecord(
            id=memory_id,
            timestamp=datetime.now().isoformat(),
            memory_type=memory_type,
            importance=importance,
            content=content,
            embeddings=embeddings,
            tags=tags,
            connections=[],
            access_count=0,
            last_accessed=datetime.now().isoformat(),
            retention_score=1.0
        )
        
        # ìºì‹œì— ì €ì¥
        self.memory_cache[memory_id] = memory
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        self._save_to_database(memory)
        
        # ì—°ê´€ì„± ë¶„ì„ ë° ì—°ê²°
        self._analyze_and_connect(memory)
        
        # ê²€ìƒ‰ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self._update_search_index(memory)
        
        # ìºì‹œ í¬ê¸° ê´€ë¦¬
        self._manage_cache_size()
        
        return memory_id
    
    def _generate_embeddings(self, text: str) -> List[float]:
        """ì„ë² ë”© ìƒì„± (ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ë²¡í„°)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” transformer ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
        hash_obj = hashlib.sha256(text.encode())
        hash_bytes = hash_obj.digest()
        
        # 256ë¹„íŠ¸ í•´ì‹œë¥¼ 64ê°œì˜ floatë¡œ ë³€í™˜
        embeddings = []
        for i in range(0, len(hash_bytes), 4):
            if i + 4 <= len(hash_bytes):
                int_val = int.from_bytes(hash_bytes[i:i+4], byteorder='big')
                normalized_val = (int_val / (2**32 - 1)) * 2 - 1  # -1 ~ 1 ì‚¬ì´ë¡œ ì •ê·œí™”
                embeddings.append(normalized_val)
        
        return embeddings[:64]  # 64ì°¨ì› ë²¡í„°
    
    def _save_to_database(self, memory: MemoryRecord):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë©”ëª¨ë¦¬ ì €ì¥"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            embeddings_blob = pickle.dumps(memory.embeddings) if memory.embeddings else None
            
            cursor.execute("""
                INSERT OR REPLACE INTO memories 
                (id, timestamp, memory_type, importance, content_json, embeddings_blob, 
                 tags_json, connections_json, access_count, last_accessed, retention_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                memory.id,
                memory.timestamp,
                memory.memory_type.value,
                memory.importance.value,
                json.dumps(memory.content, ensure_ascii=False),
                embeddings_blob,
                json.dumps(memory.tags, ensure_ascii=False),
                json.dumps(memory.connections, ensure_ascii=False),
                memory.access_count,
                memory.last_accessed,
                memory.retention_score
            ))
            
            conn.commit()
    
    def _analyze_and_connect(self, memory: MemoryRecord):
        """ë©”ëª¨ë¦¬ ì—°ê´€ì„± ë¶„ì„ ë° ì—°ê²°"""
        if not memory.embeddings:
            return
        
        memory_embedding = np.array(memory.embeddings)
        connections = []
        
        # ìºì‹œì˜ ë‹¤ë¥¸ ë©”ëª¨ë¦¬ë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
        for other_id, other_memory in self.memory_cache.items():
            if other_id == memory.id or not other_memory.embeddings:
                continue
            
            other_embedding = np.array(other_memory.embeddings)
            similarity = self._calculate_similarity(memory_embedding, other_embedding)
            
            # ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ ì—°ê²°
            if similarity > 0.7:
                connections.append(other_id)
                
                # ì—°ê´€ì„± ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
                if memory.id not in self.association_graph:
                    self.association_graph[memory.id] = {}
                self.association_graph[memory.id][other_id] = similarity
                
                # ì–‘ë°©í–¥ ì—°ê²°
                if other_id not in self.association_graph:
                    self.association_graph[other_id] = {}
                self.association_graph[other_id][memory.id] = similarity
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê´€ì„± ì €ì¥
                self._save_association(memory.id, other_id, similarity)
        
        # ë©”ëª¨ë¦¬ ì—°ê²° ì •ë³´ ì—…ë°ì´íŠ¸
        memory.connections = connections
        self._save_to_database(memory)
    
    def _calculate_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)"""
        try:
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
        except:
            return 0.0
    
    def _save_association(self, memory_id_1: str, memory_id_2: str, strength: float):
        """ì—°ê´€ì„± ì •ë³´ ì €ì¥"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO associations 
                (memory_id_1, memory_id_2, strength, created_at)
                VALUES (?, ?, ?, ?)
            """, (memory_id_1, memory_id_2, strength, datetime.now().isoformat()))
            conn.commit()
    
    def _update_search_index(self, memory: MemoryRecord):
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        content_str = json.dumps(memory.content, ensure_ascii=False)
        keywords = self._extract_keywords(content_str)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            for keyword, relevance in keywords.items():
                cursor.execute("""
                    INSERT OR REPLACE INTO search_index 
                    (keyword, memory_id, relevance)
                    VALUES (?, ?, ?)
                """, (keyword, memory.id, relevance))
                
                # ì¸ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
                if keyword not in self.search_index:
                    self.search_index[keyword] = []
                if memory.id not in self.search_index[keyword]:
                    self.search_index[keyword].append(memory.id)
            
            conn.commit()
    
    def _extract_keywords(self, text: str) -> Dict[str, float]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥)
        import re
        
        # íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
        cleaned_text = re.sub(r'[^\w\sê°€-í£]', ' ', text.lower())
        words = cleaned_text.split()
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_freq = {}
        for word in words:
            if len(word) > 2:  # 2ê¸€ì ì´ìƒë§Œ
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚° (ë¹ˆë„ ê¸°ë°˜)
        max_freq = max(word_freq.values()) if word_freq else 1
        keywords = {word: freq / max_freq for word, freq in word_freq.items()}
        
        # ìƒìœ„ 20ê°œ í‚¤ì›Œë“œë§Œ ì„ íƒ
        sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:20]
        
        return dict(sorted_keywords)
    
    def retrieve_memory(self, memory_id: str) -> Optional[MemoryRecord]:
        """ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
        # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        if memory_id in self.memory_cache:
            memory = self.memory_cache[memory_id]
            memory.access_count += 1
            memory.last_accessed = datetime.now().isoformat()
            return memory
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
        memory = self._load_from_database(memory_id)
        if memory:
            memory.access_count += 1
            memory.last_accessed = datetime.now().isoformat()
            self.memory_cache[memory_id] = memory
            self._save_to_database(memory)
        
        return memory
    
    def search_memories(self, 
                       query: str, 
                       memory_type: Optional[MemoryType] = None,
                       limit: int = 10) -> List[MemoryRecord]:
        """ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        query_keywords = self._extract_keywords(query)
        candidate_ids = set()
        
        for keyword in query_keywords.keys():
            if keyword in self.search_index:
                candidate_ids.update(self.search_index[keyword])
        
        # í›„ë³´ ë©”ëª¨ë¦¬ë“¤ ë¡œë“œ ë° ì ìˆ˜ ê³„ì‚°
        candidates = []
        for memory_id in candidate_ids:
            memory = self.retrieve_memory(memory_id)
            if memory and (memory_type is None or memory.memory_type == memory_type):
                score = self._calculate_search_score(memory, query_keywords)
                candidates.append((memory, score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        candidates.sort(key=lambda x: x[1], reverse=True)
        
        return [memory for memory, score in candidates[:limit]]
    
    def _calculate_search_score(self, memory: MemoryRecord, query_keywords: Dict[str, float]) -> float:
        """ê²€ìƒ‰ ì ìˆ˜ ê³„ì‚°"""
        content_str = json.dumps(memory.content, ensure_ascii=False).lower()
        
        score = 0.0
        for keyword, relevance in query_keywords.items():
            if keyword in content_str:
                score += relevance
        
        # ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
        score *= memory.importance.value / 5.0
        
        # ìµœê·¼ ì ‘ê·¼ ê°€ì¤‘ì¹˜
        last_accessed = datetime.fromisoformat(memory.last_accessed)
        days_ago = (datetime.now() - last_accessed).days
        recency_weight = max(0.1, 1.0 - (days_ago / 365))  # 1ë…„ í›„ 0.1 ê°€ì¤‘ì¹˜
        score *= recency_weight
        
        return score
    
    def get_connected_memories(self, memory_id: str, max_depth: int = 2) -> List[MemoryRecord]:
        """ì—°ê²°ëœ ë©”ëª¨ë¦¬ë“¤ ê°€ì ¸ì˜¤ê¸°"""
        connected = []
        visited = set()
        queue = [(memory_id, 0)]
        
        while queue:
            current_id, depth = queue.pop(0)
            
            if current_id in visited or depth > max_depth:
                continue
            
            visited.add(current_id)
            memory = self.retrieve_memory(current_id)
            
            if memory and current_id != memory_id:
                connected.append(memory)
            
            # ì—°ê²°ëœ ë©”ëª¨ë¦¬ë“¤ì„ íì— ì¶”ê°€
            if current_id in self.association_graph:
                for connected_id, strength in self.association_graph[current_id].items():
                    if connected_id not in visited and strength > 0.5:
                        queue.append((connected_id, depth + 1))
        
        return connected
    
    def _load_from_database(self, memory_id: str) -> Optional[MemoryRecord]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë©”ëª¨ë¦¬ ë¡œë“œ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM memories WHERE id = ?", (memory_id,))
            row = cursor.fetchone()
            
            if row:
                embeddings = pickle.loads(row[5]) if row[5] else None
                
                memory = MemoryRecord(
                    id=row[0],
                    timestamp=row[1],
                    memory_type=MemoryType(row[2]),
                    importance=MemoryImportance(row[3]),
                    content=json.loads(row[4]),
                    embeddings=embeddings,
                    tags=json.loads(row[6]),
                    connections=json.loads(row[7]),
                    access_count=row[8],
                    last_accessed=row[9],
                    retention_score=row[10]
                )
                
                return memory
        
        return None
    
    def _load_existing_memories(self):
        """ê¸°ì¡´ ë©”ëª¨ë¦¬ë“¤ ë¡œë“œ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id FROM memories ORDER BY last_accessed DESC LIMIT ?", (self.cache_size_limit,))
            
            for row in cursor.fetchall():
                memory_id = row[0]
                memory = self._load_from_database(memory_id)
                if memory:
                    self.memory_cache[memory_id] = memory
        
        # ì—°ê´€ì„± ê·¸ë˜í”„ ë¡œë“œ
        self._load_association_graph()
        
        # ê²€ìƒ‰ ì¸ë±ìŠ¤ ë¡œë“œ
        self._load_search_index()
    
    def _load_association_graph(self):
        """ì—°ê´€ì„± ê·¸ë˜í”„ ë¡œë“œ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT memory_id_1, memory_id_2, strength FROM associations")
            
            for row in cursor.fetchall():
                id1, id2, strength = row
                
                if id1 not in self.association_graph:
                    self.association_graph[id1] = {}
                self.association_graph[id1][id2] = strength
    
    def _load_search_index(self):
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ ë¡œë“œ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT keyword, memory_id FROM search_index")
            
            for row in cursor.fetchall():
                keyword, memory_id = row
                
                if keyword not in self.search_index:
                    self.search_index[keyword] = []
                if memory_id not in self.search_index[keyword]:
                    self.search_index[keyword].append(memory_id)
    
    def _manage_cache_size(self):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬"""
        if len(self.memory_cache) > self.cache_size_limit:
            # LRU ë°©ì‹ìœ¼ë¡œ ìºì‹œ ì •ë¦¬
            sorted_memories = sorted(
                self.memory_cache.items(),
                key=lambda x: (x[1].last_accessed, x[1].access_count),
                reverse=True
            )
            
            # ìƒìœ„ 80%ë§Œ ìœ ì§€
            keep_count = int(self.cache_size_limit * 0.8)
            new_cache = dict(sorted_memories[:keep_count])
            self.memory_cache = new_cache
    
    def _memory_management_loop(self):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ ë£¨í”„ (ë°±ê·¸ë¼ìš´ë“œ)"""
        while self.memory_manager_active:
            try:
                time.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
                
                self._optimize_storage()
                self._update_retention_scores()
                self._compress_old_memories()
                
                print("ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
                
            except Exception as e:
                print(f"âš ï¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì˜¤ë¥˜: {e}")
                time.sleep(300)  # 5ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„
    
    def _optimize_storage(self):
        """ì €ì¥ì†Œ ìµœì í™”"""
        # SQLite VACUUMìœ¼ë¡œ ë””ìŠ¤í¬ ê³µê°„ ìµœì í™”
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("VACUUM")
    
    def _update_retention_scores(self):
        """ë³´ì¡´ ì ìˆ˜ ì—…ë°ì´íŠ¸"""
        current_time = datetime.now()
        
        for memory in self.memory_cache.values():
            last_accessed = datetime.fromisoformat(memory.last_accessed)
            days_since_access = (current_time - last_accessed).days
            
            # ì ‘ê·¼ ë¹ˆë„ì™€ ì¤‘ìš”ë„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
            base_score = memory.importance.value / 5.0
            access_score = min(memory.access_count / 100, 1.0)
            recency_score = max(0.1, 1.0 - (days_since_access / 365))
            
            memory.retention_score = (base_score * 0.4 + access_score * 0.3 + recency_score * 0.3)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            self._save_to_database(memory)
    
    def _compress_old_memories(self):
        """ì˜¤ë˜ëœ ë©”ëª¨ë¦¬ ì••ì¶•"""
        # 1ë…„ ì´ìƒ ëœ ë©”ëª¨ë¦¬ ì¤‘ ë‚®ì€ ë³´ì¡´ ì ìˆ˜ ë©”ëª¨ë¦¬ë“¤ ì••ì¶•
        cutoff_date = datetime.now() - timedelta(days=365)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT id, content_json FROM memories 
                WHERE timestamp < ? AND retention_score < 0.3 AND importance < 3
            """, (cutoff_date.isoformat(),))
            
            compress_count = 0
            for row in cursor.fetchall():
                memory_id, content_json = row
                
                # ë‚´ìš© ì••ì¶•
                compressed_content = gzip.compress(content_json.encode())
                
                # ì••ì¶•ëœ ë‚´ìš©ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë³„ë„ í…Œì´ë¸” ì‚¬ìš© ê¶Œì¥)
                cursor.execute("""
                    UPDATE memories SET content_json = ? WHERE id = ?
                """, (f"COMPRESSED:{compressed_content.hex()}", memory_id))
                
                compress_count += 1
            
            conn.commit()
            
            if compress_count > 0:
                print(f"ğŸ’¾ {compress_count}ê°œ ë©”ëª¨ë¦¬ ì••ì¶• ì™„ë£Œ")
    
    def get_memory_statistics(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ì „ì²´ ë©”ëª¨ë¦¬ ìˆ˜
            cursor.execute("SELECT COUNT(*) FROM memories")
            total_count = cursor.fetchone()[0]
            
            # ë©”ëª¨ë¦¬ íƒ€ì…ë³„ ë¶„í¬
            cursor.execute("SELECT memory_type, COUNT(*) FROM memories GROUP BY memory_type")
            type_distribution = dict(cursor.fetchall())
            
            # ì¤‘ìš”ë„ë³„ ë¶„í¬
            cursor.execute("SELECT importance, COUNT(*) FROM memories GROUP BY importance")
            importance_distribution = dict(cursor.fetchall())
            
            # í‰ê·  ì ‘ê·¼ íšŸìˆ˜
            cursor.execute("SELECT AVG(access_count) FROM memories")
            avg_access = cursor.fetchone()[0] or 0
            
            # ìµœê·¼ 1ì£¼ì¼ ìƒì„± ë©”ëª¨ë¦¬
            week_ago = (datetime.now() - timedelta(days=7)).isoformat()
            cursor.execute("SELECT COUNT(*) FROM memories WHERE timestamp > ?", (week_ago,))
            recent_count = cursor.fetchone()[0]
        
        return {
            "ì´_ë©”ëª¨ë¦¬_ìˆ˜": total_count,
            "ìºì‹œ_ë©”ëª¨ë¦¬_ìˆ˜": len(self.memory_cache),
            "íƒ€ì…ë³„_ë¶„í¬": type_distribution,
            "ì¤‘ìš”ë„ë³„_ë¶„í¬": importance_distribution,
            "í‰ê· _ì ‘ê·¼_íšŸìˆ˜": round(avg_access, 1),
            "ìµœê·¼_1ì£¼ì¼_ìƒì„±": recent_count,
            "ì—°ê´€ì„±_ì—°ê²°_ìˆ˜": len(self.association_graph),
            "ê²€ìƒ‰_í‚¤ì›Œë“œ_ìˆ˜": len(self.search_index),
            "ë°ì´í„°ë² ì´ìŠ¤_í¬ê¸°_MB": round(self.db_path.stat().st_size / (1024*1024), 2) if self.db_path.exists() else 0
        }
    
    def store_conversation(self, user_input: str, ai_response: str, context: Dict[str, Any] = None) -> str:
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì €ì¥ (í¸ì˜ ë©”ì„œë“œ)"""
        content = {
            "user_input": user_input,
            "ai_response": ai_response,
            "context": context or {},
            "conversation_id": str(uuid.uuid4()),
            "response_length": len(ai_response),
            "user_engagement": len(user_input) > 50
        }
        
        # ì¤‘ìš”ë„ ìë™ íŒì •
        importance = self._assess_conversation_importance(user_input, ai_response)
        
        # íƒœê·¸ ìë™ ìƒì„±
        tags = self._generate_conversation_tags(user_input, ai_response)
        
        return self.store_memory(
            content=content,
            memory_type=MemoryType.CONVERSATION,
            importance=importance,
            tags=tags
        )
    
    def _assess_conversation_importance(self, user_input: str, ai_response: str) -> MemoryImportance:
        """ëŒ€í™” ì¤‘ìš”ë„ í‰ê°€"""
        combined = user_input + " " + ai_response
        
        # ì¤‘ìš” í‚¤ì›Œë“œ ì²´í¬
        critical_keywords = ["í”„ë¡œì íŠ¸", "êµ¬í˜„", "ì‹œìŠ¤í…œ", "ì•„í‚¤í…ì²˜", "ì„¤ê³„"]
        high_keywords = ["ì½”ë“œ", "ê°œë°œ", "ê¸°ëŠ¥", "ìµœì í™”", "ì„±ëŠ¥"]
        
        critical_count = sum(1 for kw in critical_keywords if kw in combined)
        high_count = sum(1 for kw in high_keywords if kw in combined)
        
        if critical_count >= 2:
            return MemoryImportance.CRITICAL
        elif critical_count >= 1 or high_count >= 3:
            return MemoryImportance.HIGH
        elif high_count >= 1:
            return MemoryImportance.MEDIUM
        else:
            return MemoryImportance.LOW
    
    def _generate_conversation_tags(self, user_input: str, ai_response: str) -> List[str]:
        """ëŒ€í™” íƒœê·¸ ìë™ ìƒì„±"""
        tags = []
        combined = (user_input + " " + ai_response).lower()
        
        # ê¸°ìˆ  íƒœê·¸
        tech_tags = {
            "python": ["python", "íŒŒì´ì¬"],
            "ai": ["ai", "artificial", "intelligence", "ì¸ê³µì§€ëŠ¥"],
            "web": ["web", "html", "css", "javascript", "ì›¹"],
            "database": ["database", "sql", "ë°ì´í„°ë² ì´ìŠ¤"],
            "api": ["api", "rest", "fastapi"],
            "algorithm": ["algorithm", "ì•Œê³ ë¦¬ì¦˜"]
        }
        
        for tag, keywords in tech_tags.items():
            if any(kw in combined for kw in keywords):
                tags.append(tag)
        
        # ê°ì •/í†¤ íƒœê·¸
        if "ê°ì‚¬" in combined or "ê³ ë§ˆì›Œ" in combined:
            tags.append("ê°ì‚¬")
        if "í˜ì‹ " in combined or "ì°½ì˜" in combined:
            tags.append("ì°½ì˜")
        if "ë¬¸ì œ" in combined and "í•´ê²°" in combined:
            tags.append("ë¬¸ì œí•´ê²°")
        
        return tags

# ì „ì—­ ë¬´í•œ ë©”ëª¨ë¦¬ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
infinite_memory = InfiniteMemoryEngine() 