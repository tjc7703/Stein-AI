import re
import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import hashlib
from dataclasses import dataclass
from enum import Enum

class QueryType(Enum):
    """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
    TECHNICAL = "technical"
    BUSINESS = "business"
    CREATIVE = "creative"
    ANALYTICAL = "analytical"
    PERSONAL = "personal"
    COMPLEX = "complex"

class DataSource(Enum):
    """ë°ì´í„° ì†ŒìŠ¤ ìœ í˜•"""
    INTERNAL = "internal"
    EXTERNAL = "external"
    CACHED = "cached"
    REALTIME = "realtime"

@dataclass
class QueryAnalysis:
    """ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼"""
    query_type: QueryType
    intent: str
    entities: List[str]
    context: Dict[str, Any]
    priority: int
    complexity: float
    data_requirements: List[str]
    suggested_sources: List[DataSource]

@dataclass
class DataApplication:
    """ë°ì´í„° ì ìš© ê²°ê³¼"""
    source: DataSource
    data: Dict[str, Any]
    relevance_score: float
    applied_at: datetime
    transformations: List[str]

class QueryAnalysisEngine:
    """ğŸ§  ê³ ê¸‰ ì§ˆë¬¸ ë¶„ì„ ë° ë°ì´í„° ì ìš© ì—”ì§„"""
    
    def __init__(self):
        self.knowledge_base = {}
        self.query_history = []
        self.context_memory = {}
        self.data_cache = {}
        self.learning_patterns = {}
        
    def analyze_query(self, query: str, user_context: Dict[str, Any] = None) -> QueryAnalysis:
        """ğŸ” ì§ˆë¬¸ ì‹¬ì¸µ ë¶„ì„"""
        
        # 1. ì˜ë„ ë¶„ì„ (Intent Analysis)
        intent = self._extract_intent(query)
        
        # 2. ê°œì²´ëª… ì¸ì‹ (Named Entity Recognition)
        entities = self._extract_entities(query)
        
        # 3. ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        query_type = self._classify_query_type(query, entities)
        
        # 4. ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        context = self._analyze_context(query, user_context)
        
        # 5. ë³µì¡ë„ ê³„ì‚°
        complexity = self._calculate_complexity(query, entities)
        
        # 6. ìš°ì„ ìˆœìœ„ ì„¤ì •
        priority = self._calculate_priority(query_type, complexity, context)
        
        # 7. ë°ì´í„° ìš”êµ¬ì‚¬í•­ ë¶„ì„
        data_requirements = self._analyze_data_requirements(query, entities)
        
        # 8. ë°ì´í„° ì†ŒìŠ¤ ì œì•ˆ
        suggested_sources = self._suggest_data_sources(data_requirements, context)
        
        analysis = QueryAnalysis(
            query_type=query_type,
            intent=intent,
            entities=entities,
            context=context,
            priority=priority,
            complexity=complexity,
            data_requirements=data_requirements,
            suggested_sources=suggested_sources
        )
        
        # íˆìŠ¤í† ë¦¬ì— ì €ì¥
        self.query_history.append({
            'query': query,
            'analysis': analysis,
            'timestamp': datetime.now()
        })
        
        return analysis
    
    def apply_data_mechanism(self, analysis: QueryAnalysis, available_data: Dict[str, Any]) -> List[DataApplication]:
        """âš™ï¸ ë°ì´í„° ì ìš© ë©”ì»¤ë‹ˆì¦˜"""
        
        applications = []
        
        for source in analysis.suggested_sources:
            # ë°ì´í„° ë§¤ì¹­
            matched_data = self._match_data(analysis.data_requirements, available_data, source)
            
            if matched_data:
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                relevance_score = self._calculate_relevance(analysis, matched_data)
                
                # ë°ì´í„° ë³€í™˜
                transformed_data, transformations = self._transform_data(matched_data, analysis)
                
                application = DataApplication(
                    source=source,
                    data=transformed_data,
                    relevance_score=relevance_score,
                    applied_at=datetime.now(),
                    transformations=transformations
                )
                
                applications.append(application)
        
        return applications
    
    def _extract_intent(self, query: str) -> str:
        """ì˜ë„ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜"""
        intent_patterns = {
            'create': r'ë§Œë“¤|ìƒì„±|êµ¬í˜„|ê°œë°œ|ì œì‘',
            'analyze': r'ë¶„ì„|ê²€í† |í‰ê°€|ì¡°ì‚¬',
            'improve': r'ê°œì„ |í–¥ìƒ|ì—…ê·¸ë ˆì´ë“œ|ìµœì í™”',
            'explain': r'ì„¤ëª…|ì–´ë–»ê²Œ|ì™œ|ë°©ë²•',
            'compare': r'ë¹„êµ|ì°¨ì´|vs|ëŒ€ë¹„',
            'predict': r'ì˜ˆì¸¡|ì „ë§|ë¯¸ë˜|ì˜ˆìƒ',
            'learn': r'í•™ìŠµ|ë°°ìš°|ì•Œê³  ì‹¶|ê¶ê¸ˆ',
            'solve': r'í•´ê²°|ë¬¸ì œ|ì˜¤ë¥˜|ë””ë²„ê¹…'
        }
        
        for intent, pattern in intent_patterns.items():
            if re.search(pattern, query):
                return intent
        
        return 'general'
    
    def _extract_entities(self, query: str) -> List[str]:
        """ê°œì²´ëª… ì¸ì‹"""
        entities = []
        
        # ê¸°ìˆ  ìš©ì–´ ì¶”ì¶œ
        tech_patterns = [
            r'AI|ì¸ê³µì§€ëŠ¥|ë¨¸ì‹ ëŸ¬ë‹|ë”¥ëŸ¬ë‹',
            r'Python|JavaScript|React|FastAPI',
            r'ì•Œê³ ë¦¬ì¦˜|ë©”ì»¤ë‹ˆì¦˜|ì‹œìŠ¤í…œ|ì—”ì§„',
            r'API|ë°ì´í„°ë² ì´ìŠ¤|ì„œë²„|í´ë¼ìš°ë“œ'
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            entities.extend(matches)
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ ì¶”ì¶œ
        business_patterns = [
            r'ë¹„ì¦ˆë‹ˆìŠ¤|ì‚¬ì—…|ìˆ˜ìµ|ë§¤ì¶œ|ê³ ê°',
            r'ë§ˆì¼€íŒ…|íŒë§¤|í™ë³´|ë¸Œëœë”©',
            r'ì˜ë¢°|í”„ë¡œì íŠ¸|ì„œë¹„ìŠ¤|ì†”ë£¨ì…˜'
        ]
        
        for pattern in business_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            entities.extend(matches)
        
        return list(set(entities))
    
    def _classify_query_type(self, query: str, entities: List[str]) -> QueryType:
        """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
        
        # ê¸°ìˆ ì  ì§ˆë¬¸
        if any(word in query for word in ['ì½”ë“œ', 'ì•Œê³ ë¦¬ì¦˜', 'êµ¬í˜„', 'ë””ë²„ê¹…', 'ì„±ëŠ¥']):
            return QueryType.TECHNICAL
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ì§ˆë¬¸
        if any(word in query for word in ['ë¹„ì¦ˆë‹ˆìŠ¤', 'ì‚¬ì—…', 'ìˆ˜ìµ', 'ì˜ë¢°', 'ê³ ê°']):
            return QueryType.BUSINESS
        
        # ì°½ì˜ì  ì§ˆë¬¸
        if any(word in query for word in ['ì•„ì´ë””ì–´', 'ì°½ì˜', 'í˜ì‹ ', 'ìƒˆë¡œìš´']):
            return QueryType.CREATIVE
        
        # ë¶„ì„ì  ì§ˆë¬¸
        if any(word in query for word in ['ë¶„ì„', 'ë¹„êµ', 'í‰ê°€', 'í†µê³„']):
            return QueryType.ANALYTICAL
        
        # ê°œì¸ì  ì§ˆë¬¸
        if any(word in query for word in ['ë‚´ê°€', 'ë‚˜ì˜', 'ê°œì¸', 'ë§ì¶¤']):
            return QueryType.PERSONAL
        
        # ë³µí•©ì  ì§ˆë¬¸ (ì—¬ëŸ¬ ì£¼ì œê°€ ì„ì„)
        if len(entities) > 5 or 'ê·¸ë¦¬ê³ ' in query:
            return QueryType.COMPLEX
        
        return QueryType.TECHNICAL
    
    def _analyze_context(self, query: str, user_context: Dict[str, Any]) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        context = {
            'user_context': user_context or {},
            'query_length': len(query),
            'question_count': query.count('?'),
            'urgency_indicators': self._detect_urgency(query),
            'emotional_tone': self._detect_emotion(query),
            'technical_level': self._assess_technical_level(query)
        }
        
        return context
    
    def _detect_urgency(self, query: str) -> List[str]:
        """ê¸´ê¸‰ë„ íƒì§€"""
        urgency_words = ['ê¸‰í•´', 'ë¹¨ë¦¬', 'ì¦‰ì‹œ', 'ì§€ê¸ˆ', 'ë‹¹ì¥', 'ì˜¤ëŠ˜']
        return [word for word in urgency_words if word in query]
    
    def _detect_emotion(self, query: str) -> str:
        """ê°ì • íƒì§€"""
        positive_words = ['ì¢‹ì•„', 'ìµœê³ ', 'í›Œë¥­', 'ê°ì‚¬', 'ê³ ë§ˆì›Œ']
        negative_words = ['ë¬¸ì œ', 'ì•ˆë¼', 'ì–´ë ¤ì›Œ', 'í˜ë“¤ì–´', 'ê±±ì •']
        
        positive_count = sum(1 for word in positive_words if word in query)
        negative_count = sum(1 for word in negative_words if word in query)
        
        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'
    
    def _assess_technical_level(self, query: str) -> str:
        """ê¸°ìˆ ì  ìˆ˜ì¤€ í‰ê°€"""
        basic_terms = ['ë§Œë“¤ì–´', 'ì–´ë–»ê²Œ', 'ë°©ë²•', 'ì‰½ê²Œ']
        advanced_terms = ['ì•Œê³ ë¦¬ì¦˜', 'ë©”ì»¤ë‹ˆì¦˜', 'ì•„í‚¤í…ì²˜', 'ìµœì í™”', 'ë¦¬íŒ©í† ë§']
        
        if any(term in query for term in advanced_terms):
            return 'advanced'
        elif any(term in query for term in basic_terms):
            return 'basic'
        else:
            return 'intermediate'
    
    def _calculate_complexity(self, query: str, entities: List[str]) -> float:
        """ë³µì¡ë„ ê³„ì‚°"""
        factors = {
            'length': len(query) / 1000,  # ë¬¸ì¥ ê¸¸ì´
            'entities': len(entities) / 10,  # ê°œì²´ ìˆ˜
            'questions': query.count('?') / 5,  # ì§ˆë¬¸ ìˆ˜
            'conjunctions': len(re.findall(r'ê·¸ë¦¬ê³ |ë˜í•œ|ê·¸ëŸ°ë°|í•˜ì§€ë§Œ', query)) / 3  # ì ‘ì†ì‚¬
        }
        
        complexity = sum(factors.values())
        return min(complexity, 1.0)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
    
    def _calculate_priority(self, query_type: QueryType, complexity: float, context: Dict[str, Any]) -> int:
        """ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        base_priority = {
            QueryType.TECHNICAL: 8,
            QueryType.BUSINESS: 7,
            QueryType.CREATIVE: 6,
            QueryType.ANALYTICAL: 7,
            QueryType.PERSONAL: 9,
            QueryType.COMPLEX: 8
        }
        
        priority = base_priority.get(query_type, 5)
        
        # ê¸´ê¸‰ë„ ë°˜ì˜
        if context.get('urgency_indicators'):
            priority += 2
        
        # ë³µì¡ë„ ë°˜ì˜
        priority += int(complexity * 3)
        
        return min(priority, 10)
    
    def _analyze_data_requirements(self, query: str, entities: List[str]) -> List[str]:
        """ë°ì´í„° ìš”êµ¬ì‚¬í•­ ë¶„ì„"""
        requirements = []
        
        # ê¸°ìˆ  ë°ì´í„°
        if any(word in query for word in ['ì½”ë“œ', 'ì•Œê³ ë¦¬ì¦˜', 'êµ¬í˜„']):
            requirements.extend(['code_examples', 'best_practices', 'documentation'])
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë°ì´í„°
        if any(word in query for word in ['ë¹„ì¦ˆë‹ˆìŠ¤', 'ì‹œì¥', 'ìˆ˜ìµ']):
            requirements.extend(['market_data', 'business_metrics', 'case_studies'])
        
        # ì„±ëŠ¥ ë°ì´í„°
        if any(word in query for word in ['ì„±ëŠ¥', 'ìµœì í™”', 'ì†ë„']):
            requirements.extend(['performance_metrics', 'benchmarks', 'optimization_tips'])
        
        return requirements
    
    def _suggest_data_sources(self, requirements: List[str], context: Dict[str, Any]) -> List[DataSource]:
        """ë°ì´í„° ì†ŒìŠ¤ ì œì•ˆ"""
        sources = []
        
        if 'code_examples' in requirements:
            sources.append(DataSource.INTERNAL)
        
        if 'market_data' in requirements:
            sources.append(DataSource.EXTERNAL)
        
        if context.get('technical_level') == 'advanced':
            sources.append(DataSource.REALTIME)
        
        sources.append(DataSource.CACHED)  # ê¸°ë³¸ì ìœ¼ë¡œ ìºì‹œ ì‚¬ìš©
        
        return list(set(sources))
    
    def _match_data(self, requirements: List[str], available_data: Dict[str, Any], source: DataSource) -> Dict[str, Any]:
        """ë°ì´í„° ë§¤ì¹­"""
        matched = {}
        
        for req in requirements:
            if req in available_data:
                matched[req] = available_data[req]
        
        return matched
    
    def _calculate_relevance(self, analysis: QueryAnalysis, data: Dict[str, Any]) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        relevance = 0.0
        
        # ë°ì´í„° í•­ëª© ìˆ˜ì— ë”°ë¥¸ ì ìˆ˜
        relevance += min(len(data) / 10, 0.5)
        
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        type_weights = {
            QueryType.TECHNICAL: 0.9,
            QueryType.BUSINESS: 0.8,
            QueryType.CREATIVE: 0.7,
            QueryType.ANALYTICAL: 0.9,
            QueryType.PERSONAL: 0.8,
            QueryType.COMPLEX: 0.7
        }
        
        relevance += type_weights.get(analysis.query_type, 0.5)
        
        return min(relevance, 1.0)
    
    def _transform_data(self, data: Dict[str, Any], analysis: QueryAnalysis) -> Tuple[Dict[str, Any], List[str]]:
        """ë°ì´í„° ë³€í™˜"""
        transformed = data.copy()
        transformations = []
        
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë³€í™˜
        if analysis.query_type == QueryType.TECHNICAL:
            # ê¸°ìˆ ì  ë°ì´í„° ê°•í™”
            transformations.append('technical_enhancement')
        
        if analysis.query_type == QueryType.BUSINESS:
            # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ì¶”ê°€
            transformations.append('business_metrics_addition')
        
        return transformed, transformations
    
    def get_learning_insights(self) -> Dict[str, Any]:
        """í•™ìŠµ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        if not self.query_history:
            return {}
        
        # ì§ˆë¬¸ íŒ¨í„´ ë¶„ì„
        query_types = [h['analysis'].query_type.value for h in self.query_history]
        most_common_type = max(set(query_types), key=query_types.count)
        
        # ë³µì¡ë„ íŠ¸ë Œë“œ
        complexities = [h['analysis'].complexity for h in self.query_history]
        avg_complexity = sum(complexities) / len(complexities)
        
        return {
            'total_queries': len(self.query_history),
            'most_common_type': most_common_type,
            'average_complexity': avg_complexity,
            'learning_patterns': self.learning_patterns,
            'improvement_suggestions': self._generate_improvement_suggestions()
        }
    
    def _generate_improvement_suggestions(self) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        if self.query_history:
            recent_queries = self.query_history[-5:]
            
            # ë³µì¡í•œ ì§ˆë¬¸ì´ ë§ìœ¼ë©´ ë‹¨ìˆœí™” ì œì•ˆ
            if sum(h['analysis'].complexity for h in recent_queries) / len(recent_queries) > 0.7:
                suggestions.append("ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¬¼ì–´ë³´ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # ê¸°ìˆ ì  ì§ˆë¬¸ì´ ë§ìœ¼ë©´ ë¬¸ì„œí™” ì œì•ˆ
            tech_count = sum(1 for h in recent_queries if h['analysis'].query_type == QueryType.TECHNICAL)
            if tech_count > 3:
                suggestions.append("ê¸°ìˆ  ë¬¸ì„œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ë‘ì‹œë©´ ë” íš¨ìœ¨ì ì¸ ê°œë°œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        return suggestions 