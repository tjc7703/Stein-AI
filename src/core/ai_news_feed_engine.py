"""
ğŸ“° Stein AI - AI ê¸°ìˆ  ë‰´ìŠ¤ í”¼ë“œ ì—”ì§„
ìµœì‹  AI ê¸°ìˆ  ë™í–¥, í•µì‹¬ ê¸°ì‚¬, ì—°êµ¬ ë…¼ë¬¸ì„ ìë™ ìˆ˜ì§‘ ë° ì •ë¦¬
"""

import asyncio
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from dataclasses import dataclass
import sqlite3
from pathlib import Path
import aiohttp
import hashlib
from urllib.parse import urljoin, urlparse

@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    summary: str
    url: str
    source: str
    category: str
    importance_score: float
    tags: List[str]
    published_date: str
    ai_analysis: str

@dataclass
class ResearchPaper:
    """ì—°êµ¬ ë…¼ë¬¸ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    authors: List[str]
    abstract: str
    arxiv_id: str
    categories: List[str]
    significance_score: float
    practical_applications: List[str]
    published_date: str

class AINewsFeedEngine:
    """ğŸ“° AI ê¸°ìˆ  ë‰´ìŠ¤ í”¼ë“œ ì—”ì§„"""
    
    def __init__(self):
        self.db_path = "data/ai_news_feed.db"
        self.news_sources = {
            "ai_research": [
                "https://arxiv.org/list/cs.AI/recent",
                "https://arxiv.org/list/cs.LG/recent", 
                "https://arxiv.org/list/cs.CL/recent"
            ],
            "tech_news": [
                "openai", "anthropic", "google-ai", "microsoft-ai",
                "nvidia", "deepmind", "meta-ai"
            ],
            "korean_sources": [
                "AIíƒ€ì„ìŠ¤", "ì „ìì‹ ë¬¸", "ZDNetì½”ë¦¬ì•„", "ITWorld"
            ]
        }
        
        self.ai_keywords = [
            "GPT", "ChatGPT", "Claude", "Bard", "LLM", "ëŒ€í™”í˜•AI",
            "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ì‹ ê²½ë§", "íŠ¸ëœìŠ¤í¬ë¨¸",
            "OpenAI", "Anthropic", "Google AI", "Microsoft AI",
            "ììœ¨ì£¼í–‰", "ì»´í“¨í„°ë¹„ì „", "ìì—°ì–´ì²˜ë¦¬", "ìŒì„±ì¸ì‹",
            "AI ìœ¤ë¦¬", "AI ì•ˆì „ì„±", "AGI", "AI ê·œì œ"
        ]
        
        self.importance_weights = {
            "breakthrough": 1.0,
            "company_news": 0.8,
            "research": 0.9,
            "product_release": 0.7,
            "regulation": 0.6,
            "opinion": 0.4
        }
        
        self._initialize_database()
        print("ğŸ“° AI ë‰´ìŠ¤ í”¼ë“œ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")
        print("ğŸ” ì‹¤ì‹œê°„ AI ê¸°ìˆ  ë™í–¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘!")
    
    def _initialize_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        Path("data").mkdir(exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS news_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT UNIQUE,
                summary TEXT,
                url TEXT,
                source TEXT,
                category TEXT,
                importance_score REAL,
                tags TEXT,
                published_date TEXT,
                ai_analysis TEXT,
                created_at TEXT
            )
        """)
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS research_papers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT UNIQUE,
                authors TEXT,
                abstract TEXT,
                arxiv_id TEXT,
                categories TEXT,
                significance_score REAL,
                practical_applications TEXT,
                published_date TEXT,
                created_at TEXT
            )
        """)
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS trending_topics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                topic TEXT,
                mention_count INTEGER,
                trend_score REAL,
                related_articles TEXT,
                analysis_date TEXT
            )
        """)
        
        conn.commit()
        conn.close()
    
    async def fetch_latest_ai_news(self) -> List[NewsArticle]:
        """ìµœì‹  AI ë‰´ìŠ¤ ìˆ˜ì§‘"""
        print("ğŸ” ìµœì‹  AI ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        
        # ì‹¤ì œ ë‰´ìŠ¤ ëŒ€ì‹  í˜„ì¬ ì£¼ìš” AI ë™í–¥ ì‹œë®¬ë ˆì´ì…˜
        simulated_news = [
            {
                "title": "OpenAI, GPT-5 ê°œë°œ ê³µì‹ ë°œí‘œ - ì¶”ë¡  ëŠ¥ë ¥ 10ë°° í–¥ìƒ",
                "summary": "OpenAIê°€ ì°¨ì„¸ëŒ€ ì–¸ì–´ëª¨ë¸ GPT-5ì˜ ê°œë°œì„ ê³µì‹ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ëŒ€ë¹„ ì¶”ë¡  ëŠ¥ë ¥ì´ 10ë°° í–¥ìƒë˜ê³ , ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì´ í¬ê²Œ ê°•í™”ë  ì˜ˆì •ì…ë‹ˆë‹¤.",
                "url": "https://openai.com/gpt5-announcement",
                "source": "OpenAI Blog",
                "category": "breakthrough",
                "importance_score": 0.95,
                "tags": ["GPT-5", "OpenAI", "LLM", "ì¶”ë¡ ëŠ¥ë ¥"],
                "published_date": datetime.now().strftime("%Y-%m-%d"),
                "ai_analysis": "ì´ëŠ” AI ì—…ê³„ì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Stein AI ì‹œìŠ¤í…œê³¼ì˜ í†µí•© ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•´ë³¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤."
            },
            {
                "title": "Anthropic Claude 3.5 Sonnet, ì½”ë”© ë²¤ì¹˜ë§ˆí¬ 1ìœ„ ë‹¬ì„±",
                "summary": "Anthropicì˜ Claude 3.5 Sonnetì´ HumanEval ì½”ë”© ë²¤ì¹˜ë§ˆí¬ì—ì„œ 92.3%ë¥¼ ê¸°ë¡í•˜ë©° ì—…ê³„ 1ìœ„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.",
                "url": "https://anthropic.com/claude-coding-benchmark",
                "source": "Anthropic",
                "category": "research",
                "importance_score": 0.88,
                "tags": ["Claude", "Anthropic", "ì½”ë”©AI", "ë²¤ì¹˜ë§ˆí¬"],
                "published_date": datetime.now().strftime("%Y-%m-%d"),
                "ai_analysis": "ì½”ë”© AIì˜ ìƒˆë¡œìš´ ê¸°ì¤€ì ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. Stein AIì˜ ì½”ë”© ëŠ¥ë ¥ í–¥ìƒì— ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ ë°œì „ì…ë‹ˆë‹¤."
            },
            {
                "title": "êµ¬ê¸€ Gemini 2.0, ì‹¤ì‹œê°„ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ê¸°ìˆ  ê³µê°œ",
                "summary": "êµ¬ê¸€ì´ Gemini 2.0ì—ì„œ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë™ì‹œ ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì„ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.",
                "url": "https://deepmind.google/gemini-2-multimodal",
                "source": "Google DeepMind",
                "category": "product_release",
                "importance_score": 0.82,
                "tags": ["Gemini", "Google", "ë©€í‹°ëª¨ë‹¬", "ì‹¤ì‹œê°„ì²˜ë¦¬"],
                "published_date": datetime.now().strftime("%Y-%m-%d"),
                "ai_analysis": "ë©€í‹°ëª¨ë‹¬ AIì˜ ìƒˆë¡œìš´ í‘œì¤€ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. Stein AIì—ë„ ì´ëŸ¬í•œ ê¸°ëŠ¥ í†µí•©ì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            },
            {
                "title": "í•œêµ­ NAVER, í•˜ì´í¼í´ë¡œë°”X 2.0 ìì²´ ê°œë°œ ì™„ë£Œ",
                "summary": "ë„¤ì´ë²„ê°€ í•œêµ­ì–´ íŠ¹í™” ì´ˆê±°ëŒ€ AI ëª¨ë¸ í•˜ì´í¼í´ë¡œë°”X 2.0ì„ ì™„ë£Œí–ˆë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.",
                "url": "https://naver-ai.github.io/hyperclova-x-2",
                "source": "NAVER AI",
                "category": "company_news",
                "importance_score": 0.75,
                "tags": ["HyperCLOVA-X", "NAVER", "í•œêµ­ì–´AI", "K-AI"],
                "published_date": datetime.now().strftime("%Y-%m-%d"),
                "ai_analysis": "í•œêµ­ì–´ AI ìƒíƒœê³„ì— ì¤‘ìš”í•œ ë°œì „ì…ë‹ˆë‹¤. Stein AIì˜ í•œêµ­ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ê³¼ ë¹„êµ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤."
            },
            {
                "title": "AI ì—ë„ˆì§€ íš¨ìœ¨ì„± í˜ì‹  - ìƒˆë¡œìš´ Neural Architecture ë“±ì¥",
                "summary": "MIT ì—°êµ¬íŒ€ì´ ê¸°ì¡´ ëŒ€ë¹„ 90% ì ì€ ì—ë„ˆì§€ë¡œ ë™ì¼í•œ ì„±ëŠ¥ì„ ë‚´ëŠ” ìƒˆë¡œìš´ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.",
                "url": "https://arxiv.org/abs/2024.energy.efficient.ai",
                "source": "MIT AI Lab",
                "category": "research",
                "importance_score": 0.91,
                "tags": ["ì—ë„ˆì§€íš¨ìœ¨ì„±", "MIT", "ì‹ ê²½ë§êµ¬ì¡°", "ê·¸ë¦°AI"],
                "published_date": datetime.now().strftime("%Y-%m-%d"),
                "ai_analysis": "ì´ëŠ” Stein AIì˜ ì—ë„ˆì§€ íš¨ìœ¨ì„± ê°œì„ ì— ì§ì ‘ì ìœ¼ë¡œ ì ìš© ê°€ëŠ¥í•œ ì¤‘ìš”í•œ ì—°êµ¬ì…ë‹ˆë‹¤."
            },
            {
                "title": "AI ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸ ì—…ë°ì´íŠ¸ - EU AI Act ì‹œí–‰ë ¹ ë°œí‘œ",
                "summary": "ìœ ëŸ½ì—°í•©ì´ AI Actì˜ êµ¬ì²´ì ì¸ ì‹œí–‰ë ¹ì„ ë°œí‘œí•˜ë©°, AI ì‹œìŠ¤í…œì˜ íˆ¬ëª…ì„±ê³¼ ì•ˆì „ì„± ê¸°ì¤€ì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤.",
                "url": "https://eu.ai.act.implementation.2024",
                "source": "European Commission",
                "category": "regulation",
                "importance_score": 0.72,
                "tags": ["AIìœ¤ë¦¬", "EU", "AIê·œì œ", "íˆ¬ëª…ì„±"],
                "published_date": datetime.now().strftime("%Y-%m-%d"),
                "ai_analysis": "ê¸€ë¡œë²Œ AI ê°œë°œ ê¸°ì¤€ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì…ë‹ˆë‹¤. Stein AI ê°œë°œ ì‹œ ì´ëŸ¬í•œ ê¸°ì¤€ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤."
            }
        ]
        
        news_articles = []
        for news_data in simulated_news:
            article = NewsArticle(**news_data)
            news_articles.append(article)
            await self._save_article_to_db(article)
        
        print(f"âœ… {len(news_articles)}ê°œì˜ ìµœì‹  AI ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")
        return news_articles
    
    async def fetch_research_papers(self) -> List[ResearchPaper]:
        """ìµœì‹  AI ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘"""
        print("ğŸ“š ìµœì‹  AI ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")
        
        # ì£¼ìš” AI ì—°êµ¬ ë…¼ë¬¸ ì‹œë®¬ë ˆì´ì…˜
        simulated_papers = [
            {
                "title": "Scaling Laws for Self-Evolving AI Systems",
                "authors": ["Dr. Emma Chen", "Prof. Alex Kim", "Dr. Sarah Williams"],
                "abstract": "ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìê°€ì§„í™”í•˜ëŠ” AI ì‹œìŠ¤í…œì˜ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ì œì‹œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì´ ìì²´ì ìœ¼ë¡œ í•™ìŠµí•˜ê³  ë°œì „í•˜ëŠ” ê³¼ì •ì—ì„œì˜ ì„±ëŠ¥ í–¥ìƒ íŒ¨í„´ì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§í–ˆìŠµë‹ˆë‹¤.",
                "arxiv_id": "2024.1201.0001",
                "categories": ["cs.AI", "cs.LG"],
                "significance_score": 0.94,
                "practical_applications": ["ìê°€ì§„í™” AI", "ì§€ì†ì  í•™ìŠµ", "ë©”íƒ€ëŸ¬ë‹"],
                "published_date": datetime.now().strftime("%Y-%m-%d")
            },
            {
                "title": "Energy-Efficient Neural Architectures for Large Language Models",
                "authors": ["Dr. Michael Park", "Prof. Lisa Zhang"],
                "abstract": "ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ì˜ ì—ë„ˆì§€ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ìƒˆë¡œìš´ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ 85% ì ì€ ì—ë„ˆì§€ë¡œ ë™ì¼í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.",
                "arxiv_id": "2024.1201.0002", 
                "categories": ["cs.LG", "cs.CL"],
                "significance_score": 0.89,
                "practical_applications": ["ì—ë„ˆì§€ íš¨ìœ¨ì„±", "ê·¸ë¦° AI", "ì§€ì†ê°€ëŠ¥í•œ AI"],
                "published_date": datetime.now().strftime("%Y-%m-%d")
            },
            {
                "title": "Mutual Learning Frameworks in Human-AI Collaboration",
                "authors": ["Dr. Jessica Liu", "Prof. David Brown", "Dr. Kevin Lee"],
                "abstract": "ì¸ê°„ê³¼ AIê°€ ìƒí˜¸ í•™ìŠµí•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì–‘ë°©í–¥ í•™ìŠµì„ í†µí•´ ì¸ê°„ê³¼ AI ëª¨ë‘ì˜ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì—°êµ¬í–ˆìŠµë‹ˆë‹¤.",
                "arxiv_id": "2024.1201.0003",
                "categories": ["cs.AI", "cs.HC"],
                "significance_score": 0.92,
                "practical_applications": ["ì¸ê°„-AI í˜‘ì—…", "ìƒí˜¸í•™ìŠµ", "í˜‘ì—… AI"],
                "published_date": datetime.now().strftime("%Y-%m-%d")
            }
        ]
        
        research_papers = []
        for paper_data in simulated_papers:
            paper = ResearchPaper(**paper_data)
            research_papers.append(paper)
            await self._save_paper_to_db(paper)
        
        print(f"âœ… {len(research_papers)}ê°œì˜ ìµœì‹  ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ!")
        return research_papers
    
    async def analyze_trending_topics(self) -> Dict:
        """AI ê¸°ìˆ  íŠ¸ë Œë“œ ë¶„ì„"""
        print("ğŸ“Š AI ê¸°ìˆ  íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        
        # í˜„ì¬ ì£¼ìš” íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶„ì„
        trending_analysis = {
            "í•«_í‚¤ì›Œë“œ": {
                "GPT-5": {"ì–¸ê¸‰_íšŸìˆ˜": 1250, "íŠ¸ë Œë“œ_ì ìˆ˜": 0.95, "ë³€í™”ìœ¨": "+340%"},
                "ìê°€ì§„í™”AI": {"ì–¸ê¸‰_íšŸìˆ˜": 890, "íŠ¸ë Œë“œ_ì ìˆ˜": 0.88, "ë³€í™”ìœ¨": "+420%"},
                "ì—ë„ˆì§€íš¨ìœ¨AI": {"ì–¸ê¸‰_íšŸìˆ˜": 650, "íŠ¸ë Œë“œ_ì ìˆ˜": 0.79, "ë³€í™”ìœ¨": "+280%"},
                "ë©€í‹°ëª¨ë‹¬": {"ì–¸ê¸‰_íšŸìˆ˜": 1100, "íŠ¸ë Œë“œ_ì ìˆ˜": 0.85, "ë³€í™”ìœ¨": "+190%"},
                "AIìœ¤ë¦¬": {"ì–¸ê¸‰_íšŸìˆ˜": 720, "íŠ¸ë Œë“œ_ì ìˆ˜": 0.68, "ë³€í™”ìœ¨": "+150%"}
            },
            "ìƒˆë¡œìš´_ê¸°ìˆ ": [
                {
                    "ê¸°ìˆ ëª…": "Self-Evolving Neural Networks",
                    "ì„¤ëª…": "ìŠ¤ìŠ¤ë¡œ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ë©° ë°œì „í•˜ëŠ” ì‹ ê²½ë§",
                    "ì ìš©_ë¶„ì•¼": ["AutoML", "ì§€ì†ì  í•™ìŠµ", "ì ì‘í˜• AI"],
                    "Stein_AI_ì—°ê´€ì„±": "ë§¤ìš° ë†’ìŒ - ì§ì ‘ ì ìš© ê°€ëŠ¥"
                },
                {
                    "ê¸°ìˆ ëª…": "Mutual Learning Algorithms",
                    "ì„¤ëª…": "ì¸ê°„ê³¼ AIê°€ ì„œë¡œ ê°€ë¥´ì¹˜ë©° í•¨ê»˜ ì„±ì¥í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜",
                    "ì ìš©_ë¶„ì•¼": ["í˜‘ì—… AI", "ê°œì¸í™”", "êµìœ¡ AI"],
                    "Stein_AI_ì—°ê´€ì„±": "ë§¤ìš° ë†’ìŒ - í•µì‹¬ ê¸°ëŠ¥"
                },
                {
                    "ê¸°ìˆ ëª…": "Green AI Architectures",
                    "ì„¤ëª…": "ì—ë„ˆì§€ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•œ ì¹œí™˜ê²½ AI êµ¬ì¡°",
                    "ì ìš©_ë¶„ì•¼": ["ì§€ì†ê°€ëŠ¥í•œ AI", "ëª¨ë°”ì¼ AI", "ì—£ì§€ ì»´í“¨íŒ…"],
                    "Stein_AI_ì—°ê´€ì„±": "ë†’ìŒ - ìµœì í™”ì— í™œìš©"
                }
            ],
            "ì—…ê³„_ë™í–¥": {
                "OpenAI": "GPT-5 ê°œë°œ, ì¶”ë¡  ëŠ¥ë ¥ í˜ì‹ ì  í–¥ìƒ",
                "Anthropic": "Claude ì½”ë”© ëŠ¥ë ¥ ì—…ê³„ 1ìœ„ ë‹¬ì„±",
                "Google": "Gemini ë©€í‹°ëª¨ë‹¬ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê¸°ìˆ  ê³µê°œ",
                "Microsoft": "Copilot í†µí•© ìƒíƒœê³„ í™•ì¥",
                "Meta": "Llama ì˜¤í”ˆì†ŒìŠ¤ ì „ëµ ê°•í™”"
            },
            "í•œêµ­_AI_ìƒíƒœê³„": {
                "ë„¤ì´ë²„": "í•˜ì´í¼í´ë¡œë°”X 2.0 í•œêµ­ì–´ íŠ¹í™” ì„±ëŠ¥ í–¥ìƒ",
                "ì¹´ì¹´ì˜¤": "KakaoBrain ë©€í‹°ëª¨ë‹¬ AI ì—°êµ¬ ì§‘ì¤‘",
                "ì‚¼ì„±": "Bixby AI ì°¨ì„¸ëŒ€ ë²„ì „ ê°œë°œ",
                "LG": "ì—‘ì‚¬ì› AI í”Œë«í¼ ê¸°ì—…ìš© í™•ì¥"
            }
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— íŠ¸ë Œë“œ ì •ë³´ ì €ì¥
        await self._save_trends_to_db(trending_analysis)
        
        return trending_analysis
    
    async def generate_personalized_feed(self, user_interests: List[str] = None) -> Dict:
        """Steinë‹˜ ë§ì¶¤í˜• AI ë‰´ìŠ¤ í”¼ë“œ ìƒì„±"""
        if user_interests is None:
            user_interests = [
                "ìê°€ì§„í™”AI", "ê°œì¸í™”AI", "ì—ë„ˆì§€íš¨ìœ¨ì„±", "ì½”ë”©AI", 
                "í•œêµ­ì–´AI", "í˜ì‹ ê¸°ìˆ ", "AIìœ¤ë¦¬", "ìƒí˜¸í•™ìŠµ"
            ]
        
        print(f"ğŸ¯ Steinë‹˜ ë§ì¶¤í˜• ë‰´ìŠ¤ í”¼ë“œ ìƒì„± ì¤‘... (ê´€ì‹¬ì‚¬: {', '.join(user_interests)})")
        
        # ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
        latest_news = await self.fetch_latest_ai_news()
        latest_papers = await self.fetch_research_papers()
        trending = await self.analyze_trending_topics()
        
        # ê´€ì‹¬ì‚¬ ê¸°ë°˜ í•„í„°ë§ ë° ìˆœìœ„ ë§¤ê¸°ê¸°
        personalized_articles = []
        for article in latest_news:
            relevance_score = self._calculate_relevance(article, user_interests)
            if relevance_score > 0.3:  # ê´€ë ¨ì„± ì„ê³„ê°’
                article_dict = {
                    "ì œëª©": article.title,
                    "ìš”ì•½": article.summary,
                    "ì¶œì²˜": article.source,
                    "ì¤‘ìš”ë„": article.importance_score,
                    "ê´€ë ¨ì„±": relevance_score,
                    "Steinë‹˜_ë¶„ì„": article.ai_analysis,
                    "íƒœê·¸": article.tags,
                    "URL": article.url
                }
                personalized_articles.append(article_dict)
        
        # ì¤‘ìš”ë„ì™€ ê´€ë ¨ì„±ìœ¼ë¡œ ì •ë ¬
        personalized_articles.sort(
            key=lambda x: x["ì¤‘ìš”ë„"] * x["ê´€ë ¨ì„±"], 
            reverse=True
        )
        
        # ë§ì¶¤í˜• ì¶”ì²œ ì—°êµ¬ ë…¼ë¬¸
        relevant_papers = []
        for paper in latest_papers:
            paper_relevance = self._calculate_paper_relevance(paper, user_interests)
            if paper_relevance > 0.4:
                paper_dict = {
                    "ì œëª©": paper.title,
                    "ì €ì": ", ".join(paper.authors),
                    "ìš”ì•½": paper.abstract,
                    "ì‹¤ìš©_ì‘ìš©": paper.practical_applications,
                    "ì¤‘ìš”ë„": paper.significance_score,
                    "ê´€ë ¨ì„±": paper_relevance,
                    "arXiv_ID": paper.arxiv_id
                }
                relevant_papers.append(paper_dict)
        
        relevant_papers.sort(
            key=lambda x: x["ì¤‘ìš”ë„"] * x["ê´€ë ¨ì„±"],
            reverse=True
        )
        
        # Steinë‹˜ì„ ìœ„í•œ íŠ¹ë³„ ë¶„ì„
        stein_insights = self._generate_stein_insights(
            personalized_articles, relevant_papers, trending
        )
        
        feed = {
            "ìƒì„±_ì‹œê°„": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "Steinë‹˜_ì „ìš©": "ë§ì¶¤í˜• AI ê¸°ìˆ  ë™í–¥",
            "ì˜¤ëŠ˜ì˜_í•˜ì´ë¼ì´íŠ¸": personalized_articles[:3],
            "ì£¼ëª©í• _ì—°êµ¬": relevant_papers[:2],
            "íŠ¸ë Œë“œ_ë¶„ì„": {
                "í•«_í‚¤ì›Œë“œ_TOP5": list(trending["í•«_í‚¤ì›Œë“œ"].keys())[:5],
                "Stein_AI_ê´€ë ¨_ë™í–¥": stein_insights["stein_ai_relevant"],
                "ì£¼ìš”_ê¸°ìˆ _ë°œì „": stein_insights["tech_developments"]
            },
            "ì‹¤í–‰_ê°€ëŠ¥í•œ_ì•„ì´ë””ì–´": stein_insights["actionable_ideas"],
            "ë‹¤ìŒ_ì£¼_ì˜ˆì¸¡": stein_insights["next_week_predictions"],
            "ì „ì²´_ê¸°ì‚¬": personalized_articles,
            "ì „ì²´_ë…¼ë¬¸": relevant_papers
        }
        
        return feed
    
    def _calculate_relevance(self, article: NewsArticle, interests: List[str]) -> float:
        """ê¸°ì‚¬ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        relevance = 0.0
        
        # ì œëª©ê³¼ íƒœê·¸ì—ì„œ ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œ ë§¤ì¹­
        title_lower = article.title.lower()
        tags_lower = [tag.lower() for tag in article.tags]
        
        for interest in interests:
            interest_lower = interest.lower()
            
            # ì œëª©ì— í¬í•¨ë˜ë©´ ë†’ì€ ì ìˆ˜
            if interest_lower in title_lower:
                relevance += 0.4
            
            # íƒœê·¸ì— í¬í•¨ë˜ë©´ ì¤‘ê°„ ì ìˆ˜
            if any(interest_lower in tag for tag in tags_lower):
                relevance += 0.3
            
            # ìš”ì•½ì— í¬í•¨ë˜ë©´ ë‚®ì€ ì ìˆ˜
            if interest_lower in article.summary.lower():
                relevance += 0.2
        
        return min(relevance, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def _calculate_paper_relevance(self, paper: ResearchPaper, interests: List[str]) -> float:
        """ë…¼ë¬¸ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        relevance = 0.0
        
        title_lower = paper.title.lower()
        abstract_lower = paper.abstract.lower()
        applications = [app.lower() for app in paper.practical_applications]
        
        for interest in interests:
            interest_lower = interest.lower()
            
            if interest_lower in title_lower:
                relevance += 0.5
            
            if interest_lower in abstract_lower:
                relevance += 0.3
            
            if any(interest_lower in app for app in applications):
                relevance += 0.4
        
        return min(relevance, 1.0)
    
    def _generate_stein_insights(self, articles: List[Dict], papers: List[Dict], trends: Dict) -> Dict:
        """Steinë‹˜ì„ ìœ„í•œ íŠ¹ë³„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        
        stein_ai_relevant = []
        tech_developments = []
        actionable_ideas = []
        
        # Stein AIì™€ ì§ì ‘ ê´€ë ¨ëœ ë™í–¥ ì¶”ì¶œ
        for article in articles[:5]:  # ìƒìœ„ 5ê°œ ê¸°ì‚¬ë§Œ ë¶„ì„
            if any(keyword in article["ì œëª©"].lower() for keyword in ["ìê°€ì§„í™”", "ìƒí˜¸í•™ìŠµ", "ê°œì¸í™”", "ì—ë„ˆì§€íš¨ìœ¨"]):
                stein_ai_relevant.append({
                    "ê¸°ì‚¬": article["ì œëª©"],
                    "Stein_AI_ì ìš©ë°©ì•ˆ": article["Steinë‹˜_ë¶„ì„"]
                })
        
        # ì£¼ìš” ê¸°ìˆ  ë°œì „ì‚¬í•­
        for trend_item in trends["ìƒˆë¡œìš´_ê¸°ìˆ "]:
            if trend_item["Stein_AI_ì—°ê´€ì„±"] in ["ë§¤ìš° ë†’ìŒ", "ë†’ìŒ"]:
                tech_developments.append({
                    "ê¸°ìˆ ": trend_item["ê¸°ìˆ ëª…"],
                    "ì„¤ëª…": trend_item["ì„¤ëª…"],
                    "ì ìš©ê°€ëŠ¥ì„±": trend_item["Stein_AI_ì—°ê´€ì„±"]
                })
        
        # ì‹¤í–‰ ê°€ëŠ¥í•œ ì•„ì´ë””ì–´
        actionable_ideas = [
            {
                "ì•„ì´ë””ì–´": "GPT-5 ì—°ë™ ì¤€ë¹„",
                "ì„¤ëª…": "OpenAI GPT-5 ì¶œì‹œì— ëŒ€ë¹„í•œ Stein AI í†µí•© ì¤€ë¹„",
                "ìš°ì„ ìˆœìœ„": "ë†’ìŒ",
                "ì˜ˆìƒ_ê¸°ê°„": "2-3ì£¼"
            },
            {
                "ì•„ì´ë””ì–´": "ì—ë„ˆì§€ íš¨ìœ¨ì„± ìµœì í™”",
                "ì„¤ëª…": "MIT ì—°êµ¬ ê¸°ë°˜ Stein AI ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ 90% ì ˆê°",
                "ìš°ì„ ìˆœìœ„": "ì¤‘ê°„",
                "ì˜ˆìƒ_ê¸°ê°„": "1-2ê°œì›”"
            },
            {
                "ì•„ì´ë””ì–´": "í•œêµ­ì–´ AI ë²¤ì¹˜ë§ˆí‚¹",
                "ì„¤ëª…": "í•˜ì´í¼í´ë¡œë°”X 2.0ê³¼ Stein AI í•œêµ­ì–´ ì„±ëŠ¥ ë¹„êµ ë¶„ì„",
                "ìš°ì„ ìˆœìœ„": "ì¤‘ê°„",
                "ì˜ˆìƒ_ê¸°ê°„": "1ì£¼"
            }
        ]
        
        # ë‹¤ìŒ ì£¼ ì˜ˆì¸¡
        next_week_predictions = [
            "OpenAI GPT-5 ì¶”ê°€ ê¸°ìˆ  ìƒì„¸ ê³µê°œ ì˜ˆìƒ",
            "Google Gemini 2.0 ì •ì‹ ì¶œì‹œ ë°œí‘œ ê°€ëŠ¥ì„±",
            "í•œêµ­ ì •ë¶€ K-AI ì •ì±… ë°œí‘œ ì˜ˆì •",
            "EU AI Act ì¶”ê°€ ê°€ì´ë“œë¼ì¸ ë°œí‘œ"
        ]
        
        return {
            "stein_ai_relevant": stein_ai_relevant,
            "tech_developments": tech_developments,
            "actionable_ideas": actionable_ideas,
            "next_week_predictions": next_week_predictions
        }
    
    async def _save_article_to_db(self, article: NewsArticle):
        """ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                INSERT OR REPLACE INTO news_articles 
                (title, summary, url, source, category, importance_score, 
                 tags, published_date, ai_analysis, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                article.title, article.summary, article.url, article.source,
                article.category, article.importance_score, 
                json.dumps(article.tags), article.published_date,
                article.ai_analysis, datetime.now().isoformat()
            ))
            conn.commit()
        except sqlite3.Error as e:
            print(f"ê¸°ì‚¬ ì €ì¥ ì˜¤ë¥˜: {e}")
        finally:
            conn.close()
    
    async def _save_paper_to_db(self, paper: ResearchPaper):
        """ë…¼ë¬¸ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                INSERT OR REPLACE INTO research_papers
                (title, authors, abstract, arxiv_id, categories,
                 significance_score, practical_applications, published_date, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                paper.title, json.dumps(paper.authors), paper.abstract,
                paper.arxiv_id, json.dumps(paper.categories),
                paper.significance_score, json.dumps(paper.practical_applications),
                paper.published_date, datetime.now().isoformat()
            ))
            conn.commit()
        except sqlite3.Error as e:
            print(f"ë…¼ë¬¸ ì €ì¥ ì˜¤ë¥˜: {e}")
        finally:
            conn.close()
    
    async def _save_trends_to_db(self, trends: Dict):
        """íŠ¸ë Œë“œ ë¶„ì„ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            for topic, data in trends["í•«_í‚¤ì›Œë“œ"].items():
                cursor.execute("""
                    INSERT OR REPLACE INTO trending_topics
                    (topic, mention_count, trend_score, related_articles, analysis_date)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    topic, data["ì–¸ê¸‰_íšŸìˆ˜"], data["íŠ¸ë Œë“œ_ì ìˆ˜"],
                    json.dumps([]), datetime.now().isoformat()
                ))
            conn.commit()
        except sqlite3.Error as e:
            print(f"íŠ¸ë Œë“œ ì €ì¥ ì˜¤ë¥˜: {e}")
        finally:
            conn.close()

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
news_feed_engine = None

def get_news_feed_engine():
    """ë‰´ìŠ¤ í”¼ë“œ ì—”ì§„ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global news_feed_engine
    if news_feed_engine is None:
        news_feed_engine = AINewsFeedEngine()
    return news_feed_engine 