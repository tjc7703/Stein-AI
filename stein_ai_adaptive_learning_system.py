#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stein AI ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ
Steinë‹˜ì˜ ë°©ì‹ì— ë§ì¶° ìë™ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì ìš©í•˜ëŠ” ì§„ì •í•œ AI ì‹œìŠ¤í…œ
"""

import re
import json
import asyncio
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from collections import defaultdict

@dataclass
class SteinLearningPattern:
    """Steinë‹˜ í•™ìŠµ íŒ¨í„´"""
    original_request: str
    enhanced_response: str
    effectiveness_score: float
    success_rate: float
    usage_frequency: int
    context_patterns: List[str]
    timestamp: str
    feedback_score: Optional[float] = None

@dataclass
class AdaptiveResponse:
    """ì ì‘í˜• ì‘ë‹µ"""
    original: str
    enhanced: str
    confidence_score: float
    learning_patterns: List[str]
    auto_actions: List[str]
    recommendations: List[str]
    adaptation_reason: str

class SteinAIAdaptiveLearningSystem:
    """Stein AI ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.learning_patterns = []
        self.stein_preferences = self._load_stein_preferences()
        self.adaptation_history = []
        self.success_metrics = defaultdict(list)
        self.context_learning = {}
        self.auto_learning_enabled = True
        
    def _load_stein_preferences(self) -> Dict[str, Any]:
        """Steinë‹˜ ì„ í˜¸ë„ ë¡œë“œ"""
        return {
            "language": "korean",
            "style": "collaborative",
            "detail_level": "comprehensive",
            "auto_optimization": True,
            "test_inclusion": True,
            "documentation": True,
            "security_focus": True,
            "performance_focus": True,
            "learning_rate": 0.1,
            "confidence_threshold": 0.8
        }
    
    def learn_from_interaction(self, original: str, enhanced: str, feedback_score: float = None):
        """ìƒí˜¸ì‘ìš©ì—ì„œ í•™ìŠµ"""
        pattern = SteinLearningPattern(
            original_request=original,
            enhanced_response=enhanced,
            effectiveness_score=self._calculate_effectiveness(original, enhanced),
            success_rate=feedback_score if feedback_score else 0.8,
            usage_frequency=1,
            context_patterns=self._extract_context_patterns(original),
            timestamp=datetime.now().isoformat(),
            feedback_score=feedback_score
        )
        
        self.learning_patterns.append(pattern)
        self._update_success_metrics(pattern)
        self._adapt_learning_patterns()
    
    def _calculate_effectiveness(self, original: str, enhanced: str) -> float:
        """íš¨ê³¼ì„± ê³„ì‚°"""
        # ê¸°ë³¸ íš¨ê³¼ì„± ì ìˆ˜
        base_score = 0.5
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        keyword_score = self._calculate_keyword_score(original, enhanced)
        
        # ì»¨í…ìŠ¤íŠ¸ ì í•©ì„± ì ìˆ˜
        context_score = self._calculate_context_score(original, enhanced)
        
        # ë³µì¡ë„ ì í•©ì„± ì ìˆ˜
        complexity_score = self._calculate_complexity_score(original, enhanced)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        effectiveness = (keyword_score * 0.4 + context_score * 0.3 + complexity_score * 0.3)
        
        return min(1.0, max(0.0, effectiveness))
    
    def _calculate_keyword_score(self, original: str, enhanced: str) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        original_lower = original.lower()
        enhanced_lower = enhanced.lower()
        
        # Steinë‹˜ íŠ¹í™” í‚¤ì›Œë“œë“¤
        stein_keywords = [
            "í•¨ê»˜", "í˜‘ì—…", "ë¶„ì„", "ìµœì í™”", "í…ŒìŠ¤íŠ¸", "ë³´ì•ˆ", "ì„±ëŠ¥",
            "ë¬¸ì„œí™”", "ë¦¬ë·°", "ê°œì„ ", "êµ¬í˜„", "ì„¤ê³„", "ê²€ì¦"
        ]
        
        keyword_matches = sum(1 for keyword in stein_keywords if keyword in enhanced_lower)
        return min(1.0, keyword_matches / len(stein_keywords))
    
    def _calculate_context_score(self, original: str, enhanced: str) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ì í•©ì„± ì ìˆ˜ ê³„ì‚°"""
        # ì›ë³¸ ìš”ì²­ì˜ ì˜ë„ íŒŒì•…
        original_intent = self._extract_intent(original)
        enhanced_intent = self._extract_intent(enhanced)
        
        # ì˜ë„ ì¼ì¹˜ë„ ê³„ì‚°
        intent_match = self._calculate_intent_similarity(original_intent, enhanced_intent)
        
        return intent_match
    
    def _calculate_complexity_score(self, original: str, enhanced: str) -> float:
        """ë³µì¡ë„ ì í•©ì„± ì ìˆ˜ ê³„ì‚°"""
        original_complexity = len(original.split())
        enhanced_complexity = len(enhanced.split())
        
        # Steinë‹˜ì€ ìƒì„¸í•œ ì„¤ëª…ì„ ì„ í˜¸
        if enhanced_complexity > original_complexity * 2:
            return 0.9
        elif enhanced_complexity > original_complexity * 1.5:
            return 0.8
        else:
            return 0.6
    
    def _extract_intent(self, text: str) -> Dict[str, float]:
        """ì˜ë„ ì¶”ì¶œ"""
        intent_weights = {
            "code_review": 0.0,
            "bug_fixing": 0.0,
            "feature_implementation": 0.0,
            "optimization": 0.0,
            "testing": 0.0,
            "documentation": 0.0,
            "architecture": 0.0,
            "security": 0.0
        }
        
        text_lower = text.lower()
        
        # ì˜ë„ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
        if any(word in text_lower for word in ["ìˆ˜ì •", "ê³ ì³", "ê°œì„ ", "ë¦¬ë·°"]):
            intent_weights["code_review"] += 0.8
        if any(word in text_lower for word in ["ë²„ê·¸", "ì—ëŸ¬", "ì˜¤ë¥˜", "ë¬¸ì œ"]):
            intent_weights["bug_fixing"] += 0.8
        if any(word in text_lower for word in ["ê¸°ëŠ¥", "ì¶”ê°€", "êµ¬í˜„", "ê°œë°œ"]):
            intent_weights["feature_implementation"] += 0.8
        if any(word in text_lower for word in ["ìµœì í™”", "ì„±ëŠ¥", "ë¹ ë¥´ê²Œ"]):
            intent_weights["optimization"] += 0.8
        if any(word in text_lower for word in ["í…ŒìŠ¤íŠ¸", "ê²€ì¦"]):
            intent_weights["testing"] += 0.8
        if any(word in text_lower for word in ["ë¬¸ì„œ", "ì£¼ì„", "ì„¤ëª…"]):
            intent_weights["documentation"] += 0.8
        if any(word in text_lower for word in ["êµ¬ì¡°", "ì•„í‚¤í…ì²˜", "ì„¤ê³„"]):
            intent_weights["architecture"] += 0.8
        if any(word in text_lower for word in ["ë³´ì•ˆ", "ì¸ì¦", "ê¶Œí•œ"]):
            intent_weights["security"] += 0.8
        
        return intent_weights
    
    def _calculate_intent_similarity(self, intent1: Dict[str, float], intent2: Dict[str, float]) -> float:
        """ì˜ë„ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not intent1 or not intent2:
            return 0.0
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        dot_product = sum(intent1[k] * intent2[k] for k in intent1.keys())
        norm1 = sum(v**2 for v in intent1.values()) ** 0.5
        norm2 = sum(v**2 for v in intent2.values()) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _extract_context_patterns(self, text: str) -> List[str]:
        """ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´ ì¶”ì¶œ"""
        patterns = []
        text_lower = text.lower()
        
        # ê¸°ìˆ  ìŠ¤íƒ íŒ¨í„´
        if any(word in text_lower for word in ["python", "íŒŒì´ì¬"]):
            patterns.append("python_stack")
        if any(word in text_lower for word in ["javascript", "js"]):
            patterns.append("javascript_stack")
        if any(word in text_lower for word in ["react", "ë¦¬ì•¡íŠ¸"]):
            patterns.append("react_stack")
        
        # ë³µì¡ë„ íŒ¨í„´
        if any(word in text_lower for word in ["ê°„ë‹¨", "ê¸°ë³¸"]):
            patterns.append("low_complexity")
        elif any(word in text_lower for word in ["ë³µì¡", "ê³ ê¸‰"]):
            patterns.append("high_complexity")
        else:
            patterns.append("medium_complexity")
        
        # ê¸´ê¸‰ë„ íŒ¨í„´
        if any(word in text_lower for word in ["ê¸‰í•¨", "ë¹¨ë¦¬", "urgent"]):
            patterns.append("high_urgency")
        else:
            patterns.append("normal_urgency")
        
        return patterns
    
    def _update_success_metrics(self, pattern: SteinLearningPattern):
        """ì„±ê³µ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        for context_pattern in pattern.context_patterns:
            self.success_metrics[context_pattern].append({
                "effectiveness": pattern.effectiveness_score,
                "success_rate": pattern.success_rate,
                "timestamp": pattern.timestamp
            })
    
    def _adapt_learning_patterns(self):
        """í•™ìŠµ íŒ¨í„´ ì ì‘"""
        if len(self.learning_patterns) < 5:
            return
        
        # ì„±ê³µë¥ ì´ ë†’ì€ íŒ¨í„´ë“¤ ë¶„ì„
        successful_patterns = [p for p in self.learning_patterns if p.success_rate > 0.7]
        
        if successful_patterns:
            # ì„±ê³µ íŒ¨í„´ì—ì„œ ê³µí†µ ìš”ì†Œ ì¶”ì¶œ
            common_elements = self._extract_common_elements(successful_patterns)
            
            # ìƒˆë¡œìš´ íŒ¨í„´ ìƒì„±
            self._generate_adaptive_patterns(common_elements)
    
    def _extract_common_elements(self, patterns: List[SteinLearningPattern]) -> Dict[str, Any]:
        """ê³µí†µ ìš”ì†Œ ì¶”ì¶œ"""
        common_elements = {
            "keywords": defaultdict(int),
            "context_patterns": defaultdict(int),
            "response_structures": defaultdict(int)
        }
        
        for pattern in patterns:
            # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
            for word in pattern.enhanced_response.split():
                if len(word) > 2:
                    common_elements["keywords"][word] += 1
            
            # ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´ ë¹ˆë„
            for context in pattern.context_patterns:
                common_elements["context_patterns"][context] += 1
        
        return common_elements
    
    def _generate_adaptive_patterns(self, common_elements: Dict[str, Any]):
        """ì ì‘í˜• íŒ¨í„´ ìƒì„±"""
        # ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œë“¤
        top_keywords = sorted(common_elements["keywords"].items(), 
                            key=lambda x: x[1], reverse=True)[:10]
        
        # ê°€ì¥ ì„±ê³µì ì¸ ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´ë“¤
        top_contexts = sorted(common_elements["context_patterns"].items(),
                            key=lambda x: x[1], reverse=True)[:5]
        
        # ìƒˆë¡œìš´ ì ì‘í˜• íŒ¨í„´ ìƒì„±
        adaptive_pattern = {
            "keywords": [kw[0] for kw in top_keywords],
            "context_patterns": [ctx[0] for ctx in top_contexts],
            "confidence_score": 0.85,
            "generated_at": datetime.now().isoformat()
        }
        
        self.adaptation_history.append(adaptive_pattern)
    
    def generate_adaptive_response(self, request: str) -> AdaptiveResponse:
        """ì ì‘í˜• ì‘ë‹µ ìƒì„±"""
        # ê¸°ì¡´ í•™ìŠµ íŒ¨í„´ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ íŒ¨í„´ ì°¾ê¸°
        best_pattern = self._find_best_matching_pattern(request)
        
        if best_pattern and best_pattern.effectiveness_score > 0.8:
            # í•™ìŠµëœ íŒ¨í„´ ì‚¬ìš©
            enhanced_response = self._adapt_response_to_context(request, best_pattern.enhanced_response)
            confidence_score = best_pattern.effectiveness_score
            adaptation_reason = "í•™ìŠµëœ íŒ¨í„´ ì ìš©"
        else:
            # ìƒˆë¡œìš´ íŒ¨í„´ ìƒì„±
            enhanced_response = self._generate_new_response(request)
            confidence_score = 0.6
            adaptation_reason = "ìƒˆë¡œìš´ íŒ¨í„´ ìƒì„±"
        
        # ìë™ ì•¡ì…˜ ìƒì„±
        auto_actions = self._generate_auto_actions(request, enhanced_response)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(request, enhanced_response)
        
        return AdaptiveResponse(
            original=request,
            enhanced=enhanced_response,
            confidence_score=confidence_score,
            learning_patterns=self._extract_context_patterns(request),
            auto_actions=auto_actions,
            recommendations=recommendations,
            adaptation_reason=adaptation_reason
        )
    
    def _find_best_matching_pattern(self, request: str) -> Optional[SteinLearningPattern]:
        """ìµœì  ë§¤ì¹­ íŒ¨í„´ ì°¾ê¸°"""
        if not self.learning_patterns:
            return None
        
        request_intent = self._extract_intent(request)
        request_context = self._extract_context_patterns(request)
        
        best_pattern = None
        best_score = 0.0
        
        for pattern in self.learning_patterns:
            pattern_intent = self._extract_intent(pattern.original_request)
            pattern_context = pattern.context_patterns
            
            # ì˜ë„ ìœ ì‚¬ë„
            intent_similarity = self._calculate_intent_similarity(request_intent, pattern_intent)
            
            # ì»¨í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
            context_similarity = len(set(request_context) & set(pattern_context)) / max(len(request_context), len(pattern_context), 1)
            
            # ì¢…í•© ì ìˆ˜
            total_score = (intent_similarity * 0.7 + context_similarity * 0.3) * pattern.effectiveness_score
            
            if total_score > best_score:
                best_score = total_score
                best_pattern = pattern
        
        return best_pattern if best_score > 0.6 else None
    
    def _adapt_response_to_context(self, request: str, base_response: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ì— ë§ê²Œ ì‘ë‹µ ì ì‘"""
        context_patterns = self._extract_context_patterns(request)
        
        # ì»¨í…ìŠ¤íŠ¸ë³„ ì ì‘
        if "python_stack" in context_patterns:
            base_response += " Pythonì˜ best practiceë¥¼ ì ìš©í•´ì„œ êµ¬í˜„í•´ì¤˜."
        if "react_stack" in context_patterns:
            base_response += " Reactì˜ ìµœì‹  íŒ¨í„´ê³¼ TypeScriptë¥¼ í™œìš©í•´ì„œ êµ¬í˜„í•´ì¤˜."
        if "high_complexity" in context_patterns:
            base_response += " ë³µì¡í•œ ë¡œì§ì„ ëª¨ë“ˆí™”í•˜ê³  ì„¤ê³„ íŒ¨í„´ì„ ì ìš©í•´ì¤˜."
        if "high_urgency" in context_patterns:
            base_response += " ë¹ ë¥¸ í•´ê²°ì„ ìœ„í•´ í•µì‹¬ ê¸°ëŠ¥ë¶€í„° ìš°ì„  êµ¬í˜„í•´ì¤˜."
        
        return base_response
    
    def _generate_new_response(self, request: str) -> str:
        """ìƒˆë¡œìš´ ì‘ë‹µ ìƒì„±"""
        intent = self._extract_intent(request)
        context_patterns = self._extract_context_patterns(request)
        
        # ì˜ë„ë³„ ê¸°ë³¸ ì‘ë‹µ
        base_responses = {
            "code_review": "ì½”ë“œ ë¦¬ë·°í•˜ë©´ì„œ ê°œì„ ì ì„ ì°¾ì•„ë³´ì. ì„±ëŠ¥, ê°€ë…ì„±, ë³´ì•ˆì„ ëª¨ë‘ ê³ ë ¤í•´ì„œ ìµœì í™”í•´ì¤˜.",
            "bug_fixing": "ì´ ì—ëŸ¬ë¥¼ í•¨ê»˜ ë¶„ì„í•´ë³´ì. ì›ì¸ì„ ì°¾ê³  ë°©ì–´ ì½”ë“œë„ ì¶”ê°€í•´ì„œ ë¹„ìŠ·í•œ ë¬¸ì œê°€ ì¬ë°œí•˜ì§€ ì•Šë„ë¡ í•´ì¤˜.",
            "feature_implementation": "ì´ ê¸°ëŠ¥ì„ TDD ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•´ì¤˜. ë¨¼ì € í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ê³ , ê·¸ ë‹¤ìŒ êµ¬í˜„í•˜ê³ , ë§ˆì§€ë§‰ì— í†µí•© í…ŒìŠ¤íŠ¸ë„ ì¶”ê°€í•´ì¤˜.",
            "optimization": "ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ì„ í•´ë³´ê³  ë³‘ëª© ì§€ì ì„ ì°¾ì•„ì„œ ìµœì í™”í•´ì¤˜. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì‹¤í–‰ ì‹œê°„ì„ ëª¨ë‘ ê³ ë ¤í•´ì¤˜.",
            "testing": "ë‹¨ìœ„ í…ŒìŠ¤íŠ¸, í†µí•© í…ŒìŠ¤íŠ¸, E2E í…ŒìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì‘ì„±í•´ì¤˜. í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 90% ì´ìƒì„ ëª©í‘œë¡œ í•´ì¤˜.",
            "documentation": "ì½”ë“œì— ìƒì„¸í•œ ì£¼ì„ì„ ì¶”ê°€í•˜ê³ , READMEì™€ API ë¬¸ì„œë„ ì‘ì„±í•´ì¤˜. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì¤˜.",
            "architecture": "í´ë¦° ì•„í‚¤í…ì²˜ ì›ì¹™ì— ë”°ë¼ ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ê³ , ì˜ì¡´ì„± ì£¼ì…ê³¼ SOLID ì›ì¹™ì„ ì ìš©í•´ì¤˜.",
            "security": "ë³´ì•ˆ ì·¨ì•½ì ì„ ë¶„ì„í•˜ê³ , ì¸ì¦/ì¸ê°€ ì‹œìŠ¤í…œì„ ê°•í™”í•´ì¤˜. OWASP Top 10ì„ ê³ ë ¤í•´ì„œ ë³´ì•ˆì„ ê°•í™”í•´ì¤˜."
        }
        
        # ê°€ì¥ ë†’ì€ ì˜ë„ ì„ íƒ
        max_intent = max(intent.items(), key=lambda x: x[1])
        if max_intent[1] > 0.3:
            base_response = base_responses.get(max_intent[0], "ìš”ì²­ì„ ë¶„ì„í•´ì„œ ìµœì ì˜ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")
        else:
            base_response = "ìš”ì²­ì„ ë¶„ì„í•´ì„œ ìµœì ì˜ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        
        return self._adapt_response_to_context(request, base_response)
    
    def _generate_auto_actions(self, request: str, enhanced_response: str) -> List[str]:
        """ìë™ ì•¡ì…˜ ìƒì„±"""
        actions = []
        intent = self._extract_intent(request)
        
        if intent["code_review"] > 0.5:
            actions.extend([
                "ì½”ë“œ í’ˆì§ˆ ë¶„ì„ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘",
                "ë³´ì•ˆ ì·¨ì•½ì  ê²€ì‚¬ ì‹¤í–‰",
                "ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ìˆ˜í–‰"
            ])
        if intent["bug_fixing"] > 0.5:
            actions.extend([
                "ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ ë° ì›ì¸ íŒŒì•…",
                "ë””ë²„ê¹… ë° ë¬¸ì œì  ì‹ë³„",
                "ë°©ì–´ ì½”ë“œ ë° ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€"
            ])
        if intent["feature_implementation"] > 0.5:
            actions.extend([
                "ìš”êµ¬ì‚¬í•­ ë¶„ì„ ë° ì„¤ê³„",
                "í…ŒìŠ¤íŠ¸ ì½”ë“œ ë¨¼ì € ì‘ì„± (TDD)",
                "ê¸°ëŠ¥ êµ¬í˜„ ë° ìµœì í™”"
            ])
        
        return actions[:6]  # ìµœëŒ€ 6ê°œ ì•¡ì…˜
    
    def _generate_recommendations(self, request: str, enhanced_response: str) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        context_patterns = self._extract_context_patterns(request)
        
        if "python_stack" in context_patterns:
            recommendations.append("Python íƒ€ì… íŒíŠ¸ì™€ docstring ì¶”ê°€")
        if "react_stack" in context_patterns:
            recommendations.append("React Hookê³¼ TypeScript í™œìš©")
        if "high_complexity" in context_patterns:
            recommendations.append("ëª¨ë“ˆí™” ë° ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ê°•í™”")
        
        return recommendations[:3]
    
    def get_learning_status(self) -> Dict[str, Any]:
        """í•™ìŠµ ìƒíƒœ ë°˜í™˜"""
        return {
            "total_patterns": len(self.learning_patterns),
            "successful_patterns": len([p for p in self.learning_patterns if p.success_rate > 0.7]),
            "average_effectiveness": np.mean([p.effectiveness_score for p in self.learning_patterns]) if self.learning_patterns else 0,
            "adaptation_count": len(self.adaptation_history),
            "confidence_threshold": self.stein_preferences["confidence_threshold"]
        }

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    system = SteinAIAdaptiveLearningSystem()
    
    # í•™ìŠµ ë°ì´í„° ì¶”ê°€
    learning_data = [
        ("ì½”ë“œ ìˆ˜ì •í•´ì¤˜", "ì½”ë“œ ë¦¬ë·°í•˜ë©´ì„œ ê°œì„ ì ì„ ì°¾ì•„ë³´ì. ì„±ëŠ¥, ê°€ë…ì„±, ë³´ì•ˆì„ ëª¨ë‘ ê³ ë ¤í•´ì„œ ìµœì í™”í•´ì¤˜.", 0.9),
        ("ë²„ê·¸ ìˆ˜ì •", "ì´ ì—ëŸ¬ë¥¼ í•¨ê»˜ ë¶„ì„í•´ë³´ì. ì›ì¸ì„ ì°¾ê³  ë°©ì–´ ì½”ë“œë„ ì¶”ê°€í•´ì„œ ë¹„ìŠ·í•œ ë¬¸ì œê°€ ì¬ë°œí•˜ì§€ ì•Šë„ë¡ í•´ì¤˜.", 0.95),
        ("ê¸°ëŠ¥ ì¶”ê°€", "ì´ ê¸°ëŠ¥ì„ TDD ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•´ì¤˜. ë¨¼ì € í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ê³ , ê·¸ ë‹¤ìŒ êµ¬í˜„í•˜ê³ , ë§ˆì§€ë§‰ì— í†µí•© í…ŒìŠ¤íŠ¸ë„ ì¶”ê°€í•´ì¤˜.", 0.85),
        ("ìµœì í™”", "ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ì„ í•´ë³´ê³  ë³‘ëª© ì§€ì ì„ ì°¾ì•„ì„œ ìµœì í™”í•´ì¤˜. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì‹¤í–‰ ì‹œê°„ì„ ëª¨ë‘ ê³ ë ¤í•´ì¤˜.", 0.8),
        ("í…ŒìŠ¤íŠ¸", "ë‹¨ìœ„ í…ŒìŠ¤íŠ¸, í†µí•© í…ŒìŠ¤íŠ¸, E2E í…ŒìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì‘ì„±í•´ì¤˜. í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 90% ì´ìƒì„ ëª©í‘œë¡œ í•´ì¤˜.", 0.9)
    ]
    
    # í•™ìŠµ ì‹¤í–‰
    for original, enhanced, feedback in learning_data:
        system.learn_from_interaction(original, enhanced, feedback)
    
    # ì ì‘í˜• ì‘ë‹µ í…ŒìŠ¤íŠ¸
    test_requests = [
        "ì½”ë“œ ìˆ˜ì •í•´ì¤˜",
        "ë²„ê·¸ ìˆ˜ì •",
        "ìƒˆë¡œìš´ ê¸°ëŠ¥ êµ¬í˜„",
        "ì„±ëŠ¥ ê°œì„ ",
        "í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±"
    ]
    
    print("ğŸ¯ Stein AI ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    for request in test_requests:
        response = system.generate_adaptive_response(request)
        print(f"\nğŸ“ ì›ë³¸: {response.original}")
        print(f"ğŸš€ í–¥ìƒ: {response.enhanced}")
        print(f"ğŸ“Š ì‹ ë¢°ë„: {response.confidence_score:.2f}")
        print(f"ğŸ¯ ì ì‘ ì´ìœ : {response.adaptation_reason}")
        print(f"ğŸ”§ ìë™ ì•¡ì…˜: {', '.join(response.auto_actions[:3])}")
        print("-" * 40)
    
    # í•™ìŠµ ìƒíƒœ ì¶œë ¥
    status = system.get_learning_status()
    print(f"\nğŸ“ˆ í•™ìŠµ ìƒíƒœ:")
    print(f"- ì´ íŒ¨í„´: {status['total_patterns']}")
    print(f"- ì„±ê³µ íŒ¨í„´: {status['successful_patterns']}")
    print(f"- í‰ê·  íš¨ê³¼ì„±: {status['average_effectiveness']:.2f}")
    print(f"- ì ì‘ íšŸìˆ˜: {status['adaptation_count']}")

if __name__ == "__main__":
    main() 